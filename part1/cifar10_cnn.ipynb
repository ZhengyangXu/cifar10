{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants describing the training process.\n",
    "img_size_cropped =24\n",
    "num_channels = 3\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 256\n",
    "num_iterations = 2\n",
    "print_unit = 1\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "# Download and Import Data\n",
    "\n",
    "data_path = \"data/CIFAR-10/\"\n",
    "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "cifar10.maybe_download_and_extract(url=data_url, download_dir=data_path)\n",
    "\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "\n",
    "## For Debugging\n",
    "# images_train = images_train[0:100,:,:,:]\n",
    "# images_test = images_test[0:10,:,:,:]\n",
    "# labels_train = labels_train[0:100,:]\n",
    "# labels_test = labels_test[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "def pre_process_image(image, training):\n",
    "\n",
    "    if training:\n",
    "        \n",
    "        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "        \n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=img_size_cropped,\n",
    "                                                       target_width=img_size_cropped)\n",
    "\n",
    "    return image\n",
    "\n",
    "def pre_process(images, training):\n",
    "\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "def random_batch():\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "    \n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx,:,:,:]\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay\n",
    "\n",
    "    Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        stddev: standard deviation of a truncated Gaussian\n",
    "        wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "            decay is not added for this Variable.\n",
    "\n",
    "    Returns:\n",
    "        Variable Tensor\n",
    "    \"\"\"\n",
    "    var = _variable_on_cpu(\n",
    "        name,\n",
    "        shape,\n",
    "        tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored on CPU memory\n",
    "\n",
    "    Args:\n",
    "        name: name of the variable\n",
    "        shape: list of ints\n",
    "        initializer: initializer for Variable\n",
    "\n",
    "    Returns:\n",
    "        Variable Tensor\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    return var\n",
    "\n",
    "def compute_logits_cnn(x):\n",
    "    \n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "    x = tf.reshape(x, [-1, img_size_cropped, img_size_cropped, 3])\n",
    "\n",
    "    # layer_conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[5, 5, 3, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # max_pool\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "\n",
    "    # layer_conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=[5, 5, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name) \n",
    "\n",
    "    # max_pool\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    \n",
    "    # layer_fc1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        # flatten\n",
    "        reshape = tf.reshape(pool2, [-1, 6*6*64])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 256],\n",
    "                                              stddev=0.04, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [256], tf.constant_initializer(0.0))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "    # layer_fc2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[256, 128],\n",
    "                                              stddev=0.04, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [128], tf.constant_initializer(0.0))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=scope.name)\n",
    "\n",
    "    # softmax_classifier\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [128, num_classes],\n",
    "                                          stddev=1/128.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [num_classes],\n",
    "                              tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(fc2, weights), biases, name=scope.name)   \n",
    "\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def compute_cross_entropy(logits, y):\n",
    "    sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, name='cross_ent_terms')\n",
    "    cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from ./model/final_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/final_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored checkpoint from: ./model/final_model.ckpt\n",
      "Step   0: training accuracy 0.0859\n",
      "Step   0: training loss 2.3036\n",
      "Step   0: val accuracy 0.1000\n",
      "Step   0: val loss 2.3026\n",
      "Step   1: training accuracy 0.1445\n",
      "Step   1: training loss 2.3013\n",
      "Step   1: val accuracy 0.1000\n",
      "Step   1: val loss 2.3026\n",
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: b'savedmodel_12_14_14_29_3/saved_model.pb'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: b'savedmodel_12_14_14_29_3/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'savedmodel_12_14_14_29_3/saved_model.pb'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN\n",
    "now = datetime.now()\n",
    "dir_name = 'log_{0}_{1}'.format(now.month,now.day)\n",
    "export_dir = 'savedmodel_{0}_{1}_{2}_{3}_{4}'.format(now.month,now.day,now.hour,now.minute,now.second)\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 32, 32, num_channels], name='x')\n",
    "        y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "        images = pre_process(images=x, training=True)\n",
    "\n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits_cnn(images)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y_true)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits=logits, y=y_true)\n",
    "\n",
    "    with tf.name_scope('opt'):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_step = opt.minimize(loss)\n",
    "\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.histogram('logit', logits)\n",
    "        tf.summary.image('input', tf.reshape(images, [-1, img_size_cropped, img_size_cropped, num_channels]))\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "    summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "    summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "\n",
    "    try:\n",
    "        print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "        # Use TensorFlow to find the latest checkpoint - if any.\n",
    "        last_chk_path = tf.train.latest_checkpoint(checkpoint_dir='./model/')\n",
    "\n",
    "        # Try and load the data in the checkpoint.\n",
    "        saver.restore(sess, save_path=last_chk_path)\n",
    "\n",
    "        # If we get to this point, the checkpoint was successfully loaded.\n",
    "        print(\"Restored checkpoint from:\", last_chk_path)\n",
    "    except:\n",
    "        # If the above failed for some reason, simply\n",
    "        # initialize all the variables for the TensorFlow graph.\n",
    "        print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        X_batch, y_batch = random_batch()\n",
    "\n",
    "        _, summary = sess.run((train_step, summary_op),\n",
    "                                feed_dict={x: X_batch, y_true: y_batch})\n",
    "\n",
    "        if i%print_unit==0:\n",
    "            summary_writer_train.add_summary(summary, i)\n",
    "                \n",
    "            (train_error, train_accuracy, train_logits) = sess.run((loss, accuracy, logits), {x: X_batch, y_true: y_batch})\n",
    "            print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_accuracy), flush=True)\n",
    "            print(\"\\rStep {0:3d}: training loss {1:0.4f}\".format(i, train_error), flush=True)\n",
    "           \n",
    "            (val_error, val_accuracy, summary) = sess.run((loss, accuracy,summary_op), {x:images_test, y_true:labels_test})\n",
    "            print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_accuracy), flush=True)\n",
    "            print(\"\\rStep {0:3d}: val loss {1:0.4f}\".format(i, val_error), flush=True)\n",
    "            summary_writer_val.add_summary(summary, i)\n",
    "            saver.save(sess, './model/model_iter', global_step=i)\n",
    "\n",
    "    save_path = saver.save(sess, \"./model/final_model.ckpt\")\n",
    "    builder.add_meta_graph_and_variables(sess, [\"foo-tag\"])\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
